# PDFSplit Robots.txt
# PDFSplit Robots.txt - SEO Optimized Configuration

# Main directive for all search engines
User-agent: *
Allow: /

# Explicit allow directives for important pages
Allow: /guide.html
Allow: /privacy.html
Allow: /terms.html
Allow: /sitemap.html
Allow: /404.html
Allow: /blog/
Allow: /blog/pdf-splitting-tips.html

# Allow access to public assets
Allow: /src/assets/

# Disallow temporary, sensitive, or duplicate content directories
Disallow: /src/js/modules/
Disallow: /debug.html
Disallow: /deploy.sh
Disallow: /deploy-manual.sh
Disallow: /CLAUDE.md

# Disallow version control and configuration files
Disallow: /.gitignore
Disallow: /.eslintrc.json
Disallow: /.prettierrc
Disallow: /netlify.toml

# Crawler-specific directives
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: Googlebot-Mobile
Allow: /

# Sitemap location - critical for SEO
Sitemap: https://www.pdfsplit.com/sitemap.xml

# Host directive to specify preferred domain
Host: https://www.pdfsplit.com

# Note: PDFSplit is a free online PDF splitting tool. This file guides search engines to crawl and index our content effectively.